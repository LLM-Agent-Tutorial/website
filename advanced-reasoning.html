
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Enhancing Reasoning Capabilities · LLM Agent Tutorial</title>
  <link href="libs/bootstrap/bootstrap.min.css" rel="stylesheet" />
  <link rel="stylesheet" href="styles.css" />
  <script defer src="libs/bootstrap/bootstrap.bundle.min.js"></script>
  <script defer src="script.js"></script>
  <script defer id="mathjax" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body data-page="adv-reasoning" data-nav="advanced">
  <header class="site-header navbar navbar-expand-lg navbar-light bg-white">
    <div class="inner container-xxl">
      <a class="navbar-brand logo" href="index.html">LLM Agent Tutorial</a>
      <button class="navbar-toggler nav-toggle" type="button" data-bs-toggle="collapse" data-bs-target="#mainNav" aria-controls="mainNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <nav class="collapse navbar-collapse primary-nav" id="mainNav">
        <div class="navbar-nav ms-auto align-items-lg-center gap-2">
          <a class="nav-link" href="index.html" data-nav-target="home">Home</a>
          <a class="nav-link" href="foundations.html" data-nav-target="foundations">Foundations</a>
          <a class="nav-link" href="capabilities.html" data-nav-target="capabilities">Agent Examples</a>
          <a class="nav-link" href="practical-tips.html" data-nav-target="practical">Practical Tips</a>
          <a class="nav-link" href="advanced-knowledge.html" data-nav-target="advanced">Advanced Frontier</a>
          <a class="download-btn nav-download-btn d-lg-none text-center mt-2 mx-auto" href="A Beginner-Friendly Tutorial on LLM-based Agents.pdf" download>Download <span>PDF</span></a>
        </div>
      </nav>
      <a class="download-btn ms-lg-3 d-none d-lg-inline-flex" href="A Beginner-Friendly Tutorial on LLM-based Agents.pdf" download>Download <span>PDF</span></a>
    </div>
  </header>

  <div class="layout container-xxl">
    <div class="row g-4 flex-xl-nowrap">

<aside class="chapter-sidebar col-12 col-xl-3 col-xxl-3 order-2 order-xl-1">
  <ul class="chapter-links">
    <li><a data-chapter-link="home" href="index.html">Homepage</a></li>
    <li><a data-chapter-link="foundations" href="foundations.html">What You Must Know About LLM Agents</a></li>
    <li><a data-chapter-link="capabilities" href="capabilities.html">What LLM Agents Can Do and How They Work</a></li>
    <li>
      <span class="chapter-parent">Example</span>
      <ul class="sub-chapter-links">
        <li><a data-chapter-link="generative-agents" href="generative-agents.html">Generative Agents</a></li>
        <li><a data-chapter-link="ai-scientist" href="ai-scientist.html">The AI Scientist</a></li>
        <li><a data-chapter-link="cellagent" href="cellagent.html">CellAgent</a></li>
        <li><a data-chapter-link="virtual-lab" href="virtual-lab.html">The Virtual Lab</a></li>
        <li><a data-chapter-link="metagpt" href="metagpt.html">MetaGPT</a></li>
        <li><a data-chapter-link="finagent" href="finagent.html">FinAgent</a></li>
        <li><a data-chapter-link="voyager" href="voyager.html">Voyager</a></li>
        <li><a data-chapter-link="mobile-agent" href="mobile-agent.html">Mobile Agent</a></li>
        <li><a data-chapter-link="palm-saycan" href="palm-saycan.html">PaLM-SayCan</a></li>
      </ul>
    </li>
    <li><a data-chapter-link="practical" href="practical-tips.html">Practical Tips</a></li>
    <li>
      <span class="chapter-parent">Advanced Techniques: What is happening at the frontier</span>
      <ul class="sub-chapter-links">
        <li><a data-chapter-link="adv-knowledge" href="advanced-knowledge.html">Injecting Domain Knowledge</a></li>
        <li><a data-chapter-link="adv-reasoning" href="advanced-reasoning.html">Enhancing Reasoning Capabilities</a></li>
        <li><a data-chapter-link="adv-context" href="advanced-context.html">Managing Long Context</a></li>
        <li><a data-chapter-link="adv-internalize" href="advanced-internalize.html">Internalizing Agent Abilities</a></li>
        <li><a data-chapter-link="adv-safety" href="advanced-safety.html">Agent Safety</a></li>
      </ul>
    </li>
  </ul>
</aside>

    <main class="col-12 col-xl-6 col-xxl-6 order-1 order-xl-2 main-content">
      <section id="topic-overview" class="content-section">
        <h1 class="section-title" data-i18n-key="adv-reasoning.title">Enhancing Reasoning Capabilities</h1>
        <p data-i18n-key="adv-reasoning.overview.p1">
          Reasoning underpins many core agent abilities, including planning, tool use, and cooperation. Recent advances have significantly strengthened the reasoning capabilities of LLMs. For example, OpenAI's oseries and DeepSeek-R1 are LLMs specifically enhanced for reasoning. Compared with standard LLMs (such as the GPT-4 series) from the same providers, these models are explicitly trained to perform multistep thinking before producing an answer. As a result, they often demonstrate superior performance on complex tasks such as mathematics and programming, though they also tend to produce substantially longer outputs and consume more tokens.
        </p>
      </section>

      <section id="chain-of-thought" class="content-section">
        <h2 class="section-title" data-i18n-key="adv-reasoning.cot.title">Chain-of-Thought</h2>
        <p data-i18n-key="adv-reasoning.cot.p1">
          This technique is the most simple technique for enhancing a model's reasoning ability. It encourages models to generate intermediate reasoning steps by adding a simple instruction to the prompt—"Let's think step by step". This prompt guides the model to engage in multi-step reasoning before arriving at its final answer, rather than responding directly. Despite its simplicity, this method yields substantial improvements in reasoning performance. Building on this, long CoT extends the depth and complexity of reasoning traces by encouraging models to elaborate over multiple stages, simulate nested sub-decisions, and maintain coherence across extended contexts. Self-consistency aggregates reasoning from multiple CoT samples to increase reliability, while Tree-of-Thoughts (ToT)generalizes CoT by searching over multiple reasoning paths before settling on a final solution.
        </p>
      </section>
      <section id="test-time-scaling" class="content-section">
        <h2 class="section-title" data-i18n-key="adv-reasoning.tts.title">Test-Time Scaling</h2>
        <p data-i18n-key="adv-reasoning.tts.p1">
          Test Time Scaling (TTS) is another simple yet powerful strategy for enhancing the reasoning capabilities of LLMs at inference time, without requiring retraining or architectural changes. The core idea is to generate multiple reasoning samples–such as diverse CoT traces–and select the most consistent or confident output. This leverages the model's ability to explore varied reasoning paths and self-correct through redundancy. Two basic and popular TTS methods are Beam Search and Monte Carlo Tree Search (MCTS). Beam Search maintains a fixed number of top candidate sequences based on likelihood scores, promoting diversity while preserving high-probability paths. MCTS adaptively explores the reasoning space by simulating reasoning paths and using statistical heuristics to guide search toward promising paths. These techniques allow LLMs to scale their deliberative capacity, improving reliability and reducing hallucinations in tasks that demand multi-step reasoning, such as math word problems, commonsense inference, and strategic planning.
        </p>
      </section>
      <section id="reinforcement-learning" class="content-section">
        <h2 class="section-title" data-i18n-key="adv-reasoning.rl.title">Reinforcement Learning</h2>
        <p data-i18n-key="adv-reasoning.rl.p1">
          When one wants to enhance the reasoning capability of LLMs at training time, the technique of Reinforcement learning (RL) provides an almost standard approach. sRL enables the model to learn from experience by exploring alternative reasoning paths and improving through reward signals. These rewards may be verifiable, when grounded in objective correctness (e.g., comparing against labeled solutions), or they may quantify alignment with human preferences, when derived from human feedback. For example, REINFORCE is an early RL method that realized this idea in LLMs. More recently, advanced techniques have built on this foundation to offer greater stability and efficiency. Proximal Policy Optimization (PPO) emphasizes training stability, while Direct Preference Optimization (DPO) offers a way to directly align with human preference without explicit reward signals. Group Relative Policy Optimization (GRPO) enhances reasoning by contrasting the reward-gain across a relatively small group of reasoning paths, offering more computational efficiency.
        </p>
      </section>

      <p class="citation-intro" data-i18n-key="common.citationIntro">If you find this work helpful, please consider citing our paper:</p>
      <section class="citation-block">
        <pre><code>@article{hu2025llm_agents_tutorial,
  title={A Beginner-Friendly Tutorial on LLM-based Agents},
  author={Hu, Shuyue and Ren, Siyue and Chen, Yang and Mu, Chunjiang and Liu, Jinyi and Cui, Zhiyao and Zhang, Yiqun and Li, Hao and Zhou, Dongzhan and Xu, Jia and Zhang, Qiaosheng and Han, Chu and Zheng, Yan and Hao, Jianye and Wang, Zhen},
  year={2025},
  month={November},
  note={Manuscript in preparation}
}</code></pre>
      </section>
      <div class="page-nav">
        <a class="page-nav-btn" href="index.html" data-i18n-key="common.pageNav.backHome">Back to Home</a>
        <a class="page-nav-btn primary" href="advanced-context.html" data-i18n-key="common.pageNav.next">Next</a>
      </div>
    </main>
      <aside class="progress-sidebar col-12 col-xl-3 col-xxl-3 order-3 order-xl-3">
      <h2>On this page</h2>
      <ul class="progress-list">
        <li><a class="progress-link" data-target="topic-overview" href="#topic-overview" data-i18n-key="common.sidebar.overview">Overview</a></li>
          <li><a class="progress-link" data-target="chain-of-thought" href="#chain-of-thought" data-i18n-key="adv-reasoning.cot.title">Chain-of-Thought</a></li>
          <li><a class="progress-link" data-target="test-time-scaling" href="#test-time-scaling" data-i18n-key="adv-reasoning.tts.title">Test-Time Scaling</a></li>
          <li><a class="progress-link" data-target="reinforcement-learning" href="#reinforcement-learning" data-i18n-key="adv-reasoning.rl.title">Reinforcement Learning</a></li>
        </ul>
        <div class="progress-meter"><div class="progress-meter-fill"></div></div>
      </aside>
    </div>
  </div>

  <footer class="footer"> hushuyue@pjlab.org.cn</footer>
</body>
</html>
